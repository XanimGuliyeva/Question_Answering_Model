{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XanimGuliyeva/Question_Answering_Model/blob/main/Copia_di_1_Project_QA_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presentation Plan: Extractive Question Answering Using BERT, BM25, and DPR"
      ],
      "metadata": {
        "id": "Q9RJMibsizxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch faiss-cpu rank_bm25 sentence-transformers"
      ],
      "metadata": {
        "id": "M07qkC_K0NFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "00EKHoae0yc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "    for i in range(len(examples[\"question\"])):\n",
        "        answer = examples[\"answers\"][i]\n",
        "\n",
        "        if answer[\"answer_start\"] and answer[\"text\"]:\n",
        "            start_char = answer[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "            start_token = inputs.char_to_token(i, start_char) or 0\n",
        "            end_token = inputs.char_to_token(i, end_char - 1) or 0\n",
        "        else:\n",
        "            start_token, end_token = 0, 0\n",
        "\n",
        "        start_positions.append(start_token)\n",
        "        end_positions.append(end_token)\n",
        "\n",
        "    inputs.update({\"start_positions\": start_positions, \"end_positions\": end_positions})\n",
        "    return inputs\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "qjGttt5Q05j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset sizes\n",
        "TRAIN_SIZE = 5000\n",
        "VAL_SIZE = 2000  # Reduce validation size\n",
        "TEST_SIZE = 800  # New test set\n",
        "\n",
        "# Select subsets\n",
        "small_train_dataset = encoded_dataset[\"train\"].select(range(TRAIN_SIZE))\n",
        "small_val_dataset = encoded_dataset[\"validation\"].select(range(VAL_SIZE))\n",
        "test_dataset = encoded_dataset[\"validation\"].select(range(VAL_SIZE, VAL_SIZE + TEST_SIZE))\n",
        "\n",
        "print(f\"Train Samples: {len(small_train_dataset)}\")\n",
        "print(f\"Validation Samples: {len(small_val_dataset)}\")\n",
        "print(f\"Test Samples: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "THTS36PE2QSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "DDYgH0E33cBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "t-8trCEq8xM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "RZIz_zrX7PCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_callback import ProgressCallback\n",
        "from transformers import EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "mUucncUsw8-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_first_step=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.001,\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "6oGfgNl004Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [1, 2, 3]\n",
        "train_loss = [0.662400, 0.571400, 0.361100]\n",
        "val_loss = [0.448554, 0.350456, 0.394026]\n",
        "\n",
        "plt.plot(epochs, train_loss, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(epochs, val_loss, label=\"Validation Loss\", marker=\"o\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training vs. Validation Loss Over Epochs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KgZIzhnfqKXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "DJczrFKupGdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model after training\n",
        "trainer.save_model(\"./fine_tuned_bert\")\n",
        "\n",
        "# Load the fine-tuned model for inference\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"./fine_tuned_bert\", tokenizer=tokenizer)\n",
        "\n",
        "# Example Question & Context\n",
        "context = \"The Eiffel Tower is in Paris, France. It was built in 1889.\"\n",
        "question = \"Where is the Eiffel Tower?\"\n",
        "\n",
        "# Get prediction\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "# Print the predicted answer\n",
        "print(f\"Predicted Answer: {result}\")"
      ],
      "metadata": {
        "id": "EcQS2wvqobRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "b83BeC1yk3l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import nltk\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer"
      ],
      "metadata": {
        "id": "PhRq7Y5LBFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample documents for retrieval\n",
        "documents = [\n",
        "    \"The Eiffel Tower is in Paris.\",\n",
        "    \"The Statue of Liberty is in New York.\",\n",
        "    \"Mount Everest is the highest mountain.\"\n",
        "]\n",
        "\n",
        "query = \"Where is the Eiffel Tower?\"\n"
      ],
      "metadata": {
        "id": "A7_-amYqos3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### === 1. BM25 Retrieval === ###\n",
        "print(\"\\nüîπ Using BM25\")\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "query_tokens = word_tokenize(query.lower())\n",
        "bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "best_bm25_index = np.argmax(bm25_scores)\n",
        "best_bm25_match = documents[best_bm25_index]\n",
        "\n",
        "print(f\"‚úÖ BM25 Best Match: {best_bm25_match}\")\n",
        "print(f\"BM25 Scores: {bm25_scores}\")"
      ],
      "metadata": {
        "id": "ZFPqWIMfBIWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Use BERT QA on BM25 retrieved document\n",
        "bm25_result = qa_pipeline(question=query, context=best_bm25_match)\n",
        "print(f\"ü§ñ Extracted Answer (BM25): {bm25_result['answer']}\")"
      ],
      "metadata": {
        "id": "TCaWJqInn_Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load Sentence Transformer DPR model\n",
        "dpr_model = SentenceTransformer(\"facebook-dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Encode documents and query\n",
        "doc_embeddings = dpr_model.encode(documents, convert_to_numpy=True)\n",
        "query_embedding = dpr_model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "# **Normalize embeddings for dot product retrieval**\n",
        "faiss.normalize_L2(doc_embeddings)\n",
        "faiss.normalize_L2(query_embedding)\n",
        "\n",
        "# Use Inner Product Index (Instead of L2)\n",
        "index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "# Perform search to get similarity scores\n",
        "D, I = index.search(query_embedding, k=len(documents))  # Retrieve all docs\n",
        "\n",
        "# Since FAISS returns inner product similarity, we can directly use these as DPR scores\n",
        "dpr_similarity_scores = D[0]  # Extract similarity scores for the query\n",
        "\n",
        "# Print the results\n",
        "print(f\"Sentence Transformers DPR Similarity Scores: {dpr_similarity_scores}\")\n"
      ],
      "metadata": {
        "id": "iDOGGcSvBM8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Use BERT QA on DPR retrieved document\n",
        "st_dpr_result = qa_pipeline(question=query, context=best_st_match)\n",
        "print(f\"ü§ñ Extracted Answer (Sentence Transformers DPR): {st_dpr_result['answer']}\")"
      ],
      "metadata": {
        "id": "XmTrxwjcoSFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### === 3. Facebook DPR Implementation === ###\n",
        "print(\"\\nüîπ Using Facebook DPR\")\n",
        "\n",
        "# Load Facebook DPR models & tokenizers\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Encode the query\n",
        "question_inputs = question_tokenizer(query, return_tensors=\"pt\")\n",
        "question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
        "\n",
        "# Encode documents\n",
        "context_embeddings = []\n",
        "for doc in documents:\n",
        "    context_inputs = context_tokenizer(doc, return_tensors=\"pt\")\n",
        "    context_embedding = context_encoder(**context_inputs).pooler_output\n",
        "    context_embeddings.append(context_embedding)\n",
        "\n",
        "# Convert to NumPy and normalize\n",
        "context_embeddings = torch.cat(context_embeddings).detach().numpy()\n",
        "faiss.normalize_L2(context_embeddings)\n",
        "faiss.normalize_L2(question_embedding)\n",
        "\n",
        "# Compute similarity and retrieve best match\n",
        "fb_scores = np.dot(question_embedding, context_embeddings.T).squeeze(0)\n",
        "best_fb_index = np.argmax(fb_scores)\n",
        "best_fb_match = documents[best_fb_index]\n",
        "\n",
        "print(f\"‚úÖ Facebook DPR Best Match: {best_fb_match}\")\n",
        "print(f\"Facebook DPR Similarity Scores: {fb_scores}\")"
      ],
      "metadata": {
        "id": "XhE-rYJiBQfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Use BERT QA on Facebook DPR retrieved document\n",
        "fb_dpr_result = qa_pipeline(question=query, context=best_fb_match)\n",
        "print(f\"ü§ñ Extracted Answer (Facebook DPR): {fb_dpr_result['answer']}\")"
      ],
      "metadata": {
        "id": "ss-AO0a_olIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Raw BM25 Scores:\", bm25_scores)\n",
        "print(\"Raw ST-DPR Scores:\", D.flatten())\n",
        "print(\"Raw Facebook DPR Scores:\", fb_scores)\n"
      ],
      "metadata": {
        "id": "QbpkMWgfCoWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### === 4. Compare Models === ###\n",
        "print(\"\\nüîπ Comparing Models\")\n",
        "\n",
        "# Ensure indices are correctly defined\n",
        "best_bm25_index = np.argmax(bm25_scores)  # Get best BM25 document index\n",
        "best_st_index = np.argmax(D.flatten())  # Get best Sentence Transformer document index\n",
        "best_fb_index = np.argmax(fb_scores)  # Get best Facebook DPR document index\n",
        "\n",
        "# Normalize all retrieval scores across all documents\n",
        "bm25_normalized = bm25_scores / np.max(bm25_scores) if np.max(bm25_scores) != 0 else np.zeros_like(bm25_scores)\n",
        "st_normalized = D.flatten() / np.max(D.flatten()) if np.max(D.flatten()) != 0 else np.zeros_like(D.flatten())\n",
        "fb_normalized = fb_scores / np.max(fb_scores) if np.max(fb_scores) != 0 else np.zeros_like(fb_scores)\n",
        "\n",
        "# Combine scores into a dictionary (using document references)\n",
        "comparison = {\n",
        "    \"BM25\": (documents[best_bm25_index], bm25_normalized[best_bm25_index]),\n",
        "    \"Sentence Transformers DPR\": (documents[best_st_index], st_normalized[best_st_index]),\n",
        "    \"Facebook DPR\": (documents[best_fb_index], fb_normalized[best_fb_index])\n",
        "}\n",
        "\n",
        "# Determine the best model based on normalized similarity score\n",
        "best_model = max(comparison, key=lambda x: comparison[x][1])\n",
        "\n",
        "# Display results\n",
        "for model, (match, score) in comparison.items():\n",
        "    print(f\"{model}: Match = '{match}', Score = {score:.4f}\")\n",
        "\n",
        "print(f\"\\nüèÜ Best Model for Query: {best_model} (Match: {comparison[best_model][0]}, Score: {comparison[best_model][1]:.4f})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NdQoS1NtBqpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### === 5. Visualization with Normalization === ###\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define document labels\n",
        "doc_labels = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
        "\n",
        "# Construct a heatmap using normalized scores across all documents\n",
        "score_matrix = np.array([\n",
        "    bm25_normalized,  # BM25 normalized scores for all documents\n",
        "    st_normalized,    # ST-DPR normalized scores for all documents\n",
        "    fb_normalized     # Facebook DPR normalized scores for all documents\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(score_matrix, annot=True, xticklabels=doc_labels, yticklabels=[\"BM25\", \"ST-DPR\", \"FB-DPR\"], cmap=\"coolwarm\", fmt=\".4f\")\n",
        "plt.xlabel(\"Documents\")\n",
        "plt.ylabel(\"Retrieval Model\")\n",
        "plt.title(\"Retrieval Score Heatmap (With Normalization)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxTPCKfRCFE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc6b1041"
      },
      "source": [
        "!pip install --upgrade datasets"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
